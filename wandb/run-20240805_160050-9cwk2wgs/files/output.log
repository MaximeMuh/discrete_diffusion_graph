/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
Epoch 1/1000, Loss: 0.10524372523650527
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 2/1000, Loss: 0.6789583768695593
Epoch 3/1000, Loss: 2.3996007218956947
Epoch 4/1000, Loss: 0.14833557326346636
Epoch 5/1000, Loss: 0.11432425491511822
Epoch 6/1000, Loss: 0.0866371737793088
Epoch 7/1000, Loss: 0.2205123556777835
Epoch 8/1000, Loss: 0.48694533854722977
Epoch 9/1000, Loss: 0.23612606711685658
Epoch 10/1000, Loss: 0.08514158194884658
Epoch 11/1000, Loss: 0.03726142505183816
Epoch 12/1000, Loss: 0.031558762304484844
Epoch 13/1000, Loss: 0.028989930637180805
Epoch 14/1000, Loss: 0.02711499179713428
Epoch 15/1000, Loss: 0.026994557585567236
Epoch 16/1000, Loss: 0.026996050961315632
Epoch 17/1000, Loss: 0.026659521739929914
Epoch 18/1000, Loss: 0.026924863224849105
Epoch 19/1000, Loss: 0.026206471724435687
Epoch 20/1000, Loss: 0.026392168132588267
Epoch 21/1000, Loss: 0.025494887959212065
Epoch 22/1000, Loss: 0.025395909557119012
Epoch 23/1000, Loss: 0.025360591243952513
Epoch 24/1000, Loss: 0.025490127969533205
Epoch 25/1000, Loss: 0.024345246143639088
Epoch 26/1000, Loss: 0.02483795746229589
Epoch 27/1000, Loss: 0.02412099647335708
Epoch 28/1000, Loss: 0.024056640453636646
Epoch 29/1000, Loss: 0.023754053749144077
Epoch 30/1000, Loss: 0.023712835740298033
Epoch 31/1000, Loss: 0.026082276133820415
Epoch 32/1000, Loss: 0.025540173752233386
Epoch 33/1000, Loss: 0.02508100657723844
Epoch 34/1000, Loss: 0.024721876718103886
Epoch 35/1000, Loss: 0.02411317639052868
Epoch 36/1000, Loss: 0.02385819680057466
Epoch 37/1000, Loss: 0.023939978098496795
Epoch 38/1000, Loss: 0.022757174912840128
Epoch 39/1000, Loss: 0.02355352370068431
Epoch 40/1000, Loss: 0.022964669857174158
Epoch 41/1000, Loss: 0.022549727465957403
Epoch 42/1000, Loss: 0.022906549740582705
Epoch 43/1000, Loss: 0.022372241597622633
Epoch 44/1000, Loss: 0.02248595212586224
Epoch 45/1000, Loss: 0.022333648754283786
Epoch 46/1000, Loss: 0.02262210357002914
Epoch 47/1000, Loss: 0.022233638213947415
Epoch 48/1000, Loss: 0.02218300523236394
Epoch 49/1000, Loss: 0.021922507090494037
Epoch 50/1000, Loss: 0.022083861520513892
Epoch 51/1000, Loss: 0.02215989143587649
Epoch 52/1000, Loss: 0.021921999752521515
Epoch 53/1000, Loss: 0.02181312208995223
Epoch 54/1000, Loss: 0.021799104055389762
Epoch 55/1000, Loss: 0.022151934448629618
Epoch 56/1000, Loss: 0.0216780889313668
Epoch 57/1000, Loss: 0.021320444531738758
Epoch 58/1000, Loss: 0.021523948526009917
Epoch 59/1000, Loss: 0.022012022556737065
Epoch 60/1000, Loss: 0.021561832167208195
Epoch 61/1000, Loss: 0.02110617165453732
Epoch 62/1000, Loss: 0.021146182669326663
Epoch 63/1000, Loss: 0.021157353650778532
Epoch 64/1000, Loss: 0.021648134104907513
Epoch 65/1000, Loss: 0.021282670786604285
Epoch 66/1000, Loss: 0.021492864936590195
Epoch 67/1000, Loss: 0.02132250415161252
Epoch 68/1000, Loss: 0.021094166208058596
Epoch 69/1000, Loss: 0.02099738037213683
Epoch 70/1000, Loss: 0.02083410508930683
Epoch 71/1000, Loss: 0.020736530190333724
Epoch 72/1000, Loss: 0.021324535366147757
Epoch 73/1000, Loss: 0.02115252031944692
Epoch 74/1000, Loss: 0.02098633232526481
Epoch 75/1000, Loss: 0.02100368426181376
Epoch 76/1000, Loss: 0.02057014754973352
Epoch 77/1000, Loss: 0.020479635102674365
Epoch 78/1000, Loss: 0.020650438964366913
Epoch 79/1000, Loss: 0.02055499143898487
Epoch 80/1000, Loss: 0.020808810368180275
Epoch 81/1000, Loss: 0.020645566284656525
Epoch 82/1000, Loss: 0.020528658526018262
Epoch 83/1000, Loss: 0.020583697594702244
Epoch 84/1000, Loss: 0.020759468199685216
Epoch 85/1000, Loss: 0.021361342631280422
Epoch 86/1000, Loss: 0.020794403040781617
Epoch 87/1000, Loss: 0.020497356075793505
Epoch 88/1000, Loss: 0.020455587655305862
Epoch 89/1000, Loss: 0.020394316874444485
Epoch 90/1000, Loss: 0.020541075384244323
Epoch 91/1000, Loss: 0.02108030254021287
Epoch 92/1000, Loss: 0.02101277094334364
Epoch 93/1000, Loss: 0.02009076066315174
Epoch 94/1000, Loss: 0.02019523410126567
Epoch 95/1000, Loss: 0.020386021118611097
Epoch 96/1000, Loss: 0.020481314975768328
Epoch 97/1000, Loss: 0.020875453483313322
Epoch 98/1000, Loss: 0.020389803685247898
Epoch 99/1000, Loss: 0.020576270762830973
Epoch 100/1000, Loss: 0.020378527231514454
Epoch 101/1000, Loss: 0.02082629455253482
Epoch 102/1000, Loss: 0.020388484466820955
Epoch 103/1000, Loss: 0.020276509691029787
Epoch 104/1000, Loss: 0.02036480256356299
Epoch 105/1000, Loss: 0.020560856442898512
Epoch 106/1000, Loss: 0.020283076912164688
Epoch 107/1000, Loss: 0.020937120774760842
Epoch 108/1000, Loss: 0.020289634121581912
Epoch 109/1000, Loss: 0.02038994082249701
Epoch 110/1000, Loss: 0.0205891034565866
Epoch 111/1000, Loss: 0.020547156454995275
Epoch 112/1000, Loss: 0.02031059074215591
Epoch 113/1000, Loss: 0.020165971014648676
Epoch 114/1000, Loss: 0.02009829506278038
Epoch 115/1000, Loss: 0.020711865974590182
Epoch 116/1000, Loss: 0.02005968987941742
Epoch 117/1000, Loss: 0.020195729099214077
Epoch 118/1000, Loss: 0.01982273324392736
Epoch 119/1000, Loss: 0.02037790114991367
Epoch 120/1000, Loss: 0.020918658701702952
Epoch 121/1000, Loss: 0.02043918496929109
Epoch 122/1000, Loss: 0.02032985188998282
Epoch 123/1000, Loss: 0.020413415739312768
Epoch 124/1000, Loss: 0.020305760903283954
Epoch 125/1000, Loss: 0.021004644921049476
Epoch 126/1000, Loss: 0.020029364386573434
Epoch 127/1000, Loss: 0.020219934405758977
Epoch 128/1000, Loss: 0.020577007438987494
Epoch 129/1000, Loss: 0.020164197543635964
Epoch 130/1000, Loss: 0.020525721600279212
Epoch 131/1000, Loss: 0.020322499331086874
Epoch 132/1000, Loss: 0.02052101562730968
Epoch 133/1000, Loss: 0.020106112584471703
Epoch 134/1000, Loss: 0.020549268228933215
Epoch 135/1000, Loss: 0.020262684673070908
Epoch 136/1000, Loss: 0.020173701690509915
Epoch 137/1000, Loss: 0.02047704835422337
Epoch 138/1000, Loss: 0.02024156181141734
Epoch 139/1000, Loss: 0.01995061431080103
Epoch 140/1000, Loss: 0.01989176287315786
Epoch 141/1000, Loss: 0.020255891839042306
Epoch 142/1000, Loss: 0.020579378819093108
Epoch 143/1000, Loss: 0.020294085144996643
Epoch 144/1000, Loss: 0.020074416184797883
Epoch 145/1000, Loss: 0.020120178116485476
Epoch 146/1000, Loss: 0.020517144119367003
Epoch 147/1000, Loss: 0.020109489792957902
Epoch 148/1000, Loss: 0.020437563071027398
Epoch 149/1000, Loss: 0.020056656561791897
Epoch 150/1000, Loss: 0.02029714803211391
Epoch 151/1000, Loss: 0.020446861162781715
Epoch 152/1000, Loss: 0.020541684003546834
Epoch 153/1000, Loss: 0.020040442934259772
Epoch 154/1000, Loss: 0.020501975901424885
Epoch 155/1000, Loss: 0.01994824199937284
Epoch 156/1000, Loss: 0.02015901100821793
Epoch 157/1000, Loss: 0.020625959150493145
Epoch 158/1000, Loss: 0.020677920198068023
Epoch 159/1000, Loss: 0.01992740179412067
Epoch 160/1000, Loss: 0.020392713136970997
Epoch 161/1000, Loss: 0.020177491940557957
Epoch 162/1000, Loss: 0.02030818397179246
Epoch 163/1000, Loss: 0.019957649521529675
Epoch 164/1000, Loss: 0.020529293222352862
Epoch 165/1000, Loss: 0.02039448358118534
Epoch 166/1000, Loss: 0.020111319376155734
Epoch 167/1000, Loss: 0.020297199254855514
Epoch 168/1000, Loss: 0.0203506569378078
Epoch 169/1000, Loss: 0.020377994747832417
Epoch 170/1000, Loss: 0.02070177416317165
Epoch 171/1000, Loss: 0.020406011724844575
Epoch 172/1000, Loss: 0.02068076655268669
Epoch 173/1000, Loss: 0.020192541414871812
Epoch 174/1000, Loss: 0.020186033099889755
Epoch 175/1000, Loss: 0.019840722205117345
Epoch 176/1000, Loss: 0.020290828542783856
Epoch 177/1000, Loss: 0.019865961279720068
Epoch 178/1000, Loss: 0.01992253097705543
Epoch 179/1000, Loss: 0.020014507696032524
Epoch 180/1000, Loss: 0.02047134330496192
Epoch 181/1000, Loss: 0.020170363131910563
Epoch 182/1000, Loss: 0.020556105533614755
Epoch 183/1000, Loss: 0.020148353185504675
Epoch 184/1000, Loss: 0.02048798999749124
Epoch 185/1000, Loss: 0.02022000588476658
Epoch 186/1000, Loss: 0.020537378964945674
Epoch 187/1000, Loss: 0.0198800484649837
Epoch 188/1000, Loss: 0.019825414288789034
Epoch 189/1000, Loss: 0.01984663726761937
Epoch 190/1000, Loss: 0.020124106900766492
Epoch 191/1000, Loss: 0.02035177545621991
Epoch 192/1000, Loss: 0.0209101305808872
Epoch 193/1000, Loss: 0.020044491859152913
Epoch 194/1000, Loss: 0.021020881598815322
Epoch 195/1000, Loss: 0.020305278711020947
Epoch 196/1000, Loss: 0.020035938359797
Epoch 197/1000, Loss: 0.02001156029291451
Epoch 198/1000, Loss: 0.020130978664383292
Epoch 199/1000, Loss: 0.020407931180670857
Epoch 200/1000, Loss: 0.020430481527000666
Epoch 201/1000, Loss: 0.019957720302045345
Epoch 202/1000, Loss: 0.020119867054745555
Epoch 203/1000, Loss: 0.02020426746457815
Epoch 204/1000, Loss: 0.02002006839029491
Epoch 205/1000, Loss: 0.020223823143169284
Epoch 206/1000, Loss: 0.020306045189499855
Epoch 207/1000, Loss: 0.020114134764298797
Epoch 208/1000, Loss: 0.020429601427167654
Epoch 209/1000, Loss: 0.02011543931439519
Epoch 210/1000, Loss: 0.020191591000184417
Epoch 211/1000, Loss: 0.020229600137099624
Epoch 212/1000, Loss: 0.02027454855851829
Epoch 213/1000, Loss: 0.02049965364858508
Epoch 214/1000, Loss: 0.020640661008656025
Epoch 215/1000, Loss: 0.020263090496882796
Epoch 216/1000, Loss: 0.02028041146695614
Epoch 217/1000, Loss: 0.0201725906226784
Epoch 218/1000, Loss: 0.02008601580746472
Epoch 219/1000, Loss: 0.02061564289033413
Epoch 220/1000, Loss: 0.020435034297406673
Epoch 221/1000, Loss: 0.02075225696898997
Epoch 222/1000, Loss: 0.02017940441146493
Epoch 223/1000, Loss: 0.020429709926247597
Epoch 224/1000, Loss: 0.020249971887096763
Epoch 225/1000, Loss: 0.020516871940344572
Epoch 226/1000, Loss: 0.020534641342237592
Epoch 227/1000, Loss: 0.020167098380625248
Epoch 228/1000, Loss: 0.020322880474850535
Epoch 229/1000, Loss: 0.020358277019113302
Epoch 230/1000, Loss: 0.02008895971812308
Epoch 231/1000, Loss: 0.020479717291891575
Epoch 232/1000, Loss: 0.020264492835849524
Epoch 233/1000, Loss: 0.01995220547541976
Epoch 234/1000, Loss: 0.020433412631973624
Epoch 235/1000, Loss: 0.020162342116236687
Epoch 236/1000, Loss: 0.020272511756047606
Epoch 237/1000, Loss: 0.020206698216497898
Epoch 238/1000, Loss: 0.0204706487711519
Epoch 239/1000, Loss: 0.020160727202892303
Epoch 240/1000, Loss: 0.020170642994344234
Epoch 241/1000, Loss: 0.020117890555411577
Epoch 242/1000, Loss: 0.02040882338769734
Epoch 243/1000, Loss: 0.020256830379366875
Epoch 244/1000, Loss: 0.019897534977644682
Epoch 245/1000, Loss: 0.020450015552341938
Epoch 246/1000, Loss: 0.02006536116823554
Epoch 247/1000, Loss: 0.020143869565799832
Epoch 248/1000, Loss: 0.020226897671818733
Epoch 249/1000, Loss: 0.020184481516480446
Epoch 250/1000, Loss: 0.02037029340863228
Epoch 251/1000, Loss: 0.02012093481607735
Epoch 252/1000, Loss: 0.020207972964271903
Epoch 253/1000, Loss: 0.020072082057595253
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 85, in <module>
    fit(model, optimizer, dataloader, max_epoch=1000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 61, in fit
    train_losses.append(l.detach().cpu().item())
                        ^^^^^^^^^^^^^^^^
KeyboardInterrupt