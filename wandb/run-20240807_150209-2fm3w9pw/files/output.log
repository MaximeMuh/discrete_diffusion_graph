/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/2000, Loss: 42.622695446014404
New best model saved with loss: 42.622695446014404
Epoch 2/2000, Loss: 39.06852123141289
New best model saved with loss: 39.06852123141289
Epoch 3/2000, Loss: 39.16448098421097
Epoch 4/2000, Loss: 39.30142393708229
Epoch 5/2000, Loss: 39.28602546453476
Epoch 6/2000, Loss: 39.43364927172661
Epoch 7/2000, Loss: 39.18491140007973
Epoch 8/2000, Loss: 39.84777161478996
Epoch 9/2000, Loss: 39.31441476941109
Epoch 10/2000, Loss: 39.60125216841698
Epoch 11/2000, Loss: 38.87694463133812
New best model saved with loss: 38.87694463133812
Epoch 12/2000, Loss: 39.31644752621651
Epoch 13/2000, Loss: 39.34408015012741
Epoch 14/2000, Loss: 39.404674768447876
Epoch 15/2000, Loss: 39.34153142571449
Epoch 16/2000, Loss: 40.19769924879074
Epoch 17/2000, Loss: 39.381242126226425
Epoch 18/2000, Loss: 39.8480681180954
Epoch 19/2000, Loss: 39.66941463947296
Epoch 20/2000, Loss: 39.43143364787102
Epoch 21/2000, Loss: 40.05356127023697
Epoch 22/2000, Loss: 40.01626229286194
Epoch 23/2000, Loss: 39.51296055316925
Epoch 24/2000, Loss: 39.15970250964165
Epoch 25/2000, Loss: 39.42353165149689
Epoch 26/2000, Loss: 40.22922134399414
Epoch 27/2000, Loss: 39.453767240047455
Epoch 28/2000, Loss: 40.188396006822586
Epoch 29/2000, Loss: 39.64768332242966
Epoch 30/2000, Loss: 39.903678238391876
Epoch 31/2000, Loss: 39.30666619539261
Epoch 32/2000, Loss: 39.606584548950195
Epoch 33/2000, Loss: 39.23163801431656
Epoch 34/2000, Loss: 39.3528196811676
Epoch 35/2000, Loss: 39.08819457888603
Epoch 36/2000, Loss: 40.078858733177185
Epoch 37/2000, Loss: 40.09132644534111
Epoch 38/2000, Loss: 39.572900235652924
Epoch 39/2000, Loss: 40.03465035557747
Epoch 40/2000, Loss: 40.10342425107956
Epoch 41/2000, Loss: 39.62503755092621
Epoch 42/2000, Loss: 39.54712197184563
Epoch 43/2000, Loss: 39.36317139863968
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 104, in <module>
    fit(model, optimizer, dataloader, max_epoch=2000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 56, in fit
    l.backward()
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt