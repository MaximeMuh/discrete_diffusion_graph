/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/1000, Loss: 16.73344148695469
Epoch 2/1000, Loss: 20.761946320533752
Epoch 3/1000, Loss: 25.90990900993347
Epoch 4/1000, Loss: 19.35605050623417
Epoch 5/1000, Loss: 17.799129888415337
Epoch 6/1000, Loss: 17.601984679698944
Epoch 7/1000, Loss: 17.024414658546448
Epoch 8/1000, Loss: 16.994469739496708
Epoch 9/1000, Loss: 17.319954305887222
Epoch 10/1000, Loss: 17.560348242521286
Epoch 11/1000, Loss: 16.723081439733505
Epoch 12/1000, Loss: 16.699423596262932
Epoch 13/1000, Loss: 16.705227062106133
Epoch 14/1000, Loss: 16.818340003490448
Epoch 15/1000, Loss: 16.74242590367794
Epoch 16/1000, Loss: 16.451139986515045
Epoch 17/1000, Loss: 16.603677690029144
Epoch 18/1000, Loss: 16.386840507388115
Epoch 19/1000, Loss: 15.908057451248169
Epoch 20/1000, Loss: 16.35101894289255
Epoch 21/1000, Loss: 16.6154672652483
Epoch 22/1000, Loss: 16.091282680630684
Epoch 23/1000, Loss: 16.60402701050043
Epoch 24/1000, Loss: 16.495927959680557
Epoch 25/1000, Loss: 16.34590809047222
Epoch 26/1000, Loss: 15.821411229670048
Epoch 27/1000, Loss: 16.158537223935127
Epoch 28/1000, Loss: 16.143806099891663
Epoch 29/1000, Loss: 16.30777497589588
Epoch 30/1000, Loss: 15.950202822685242
Epoch 31/1000, Loss: 16.60503003001213
Epoch 32/1000, Loss: 16.44015035033226
Epoch 33/1000, Loss: 15.829732820391655
Epoch 34/1000, Loss: 16.164293609559536
Epoch 35/1000, Loss: 16.212139055132866
Epoch 36/1000, Loss: 15.943849012255669
Epoch 37/1000, Loss: 15.802052438259125
Epoch 38/1000, Loss: 16.040096171200275
Epoch 39/1000, Loss: 16.16253999620676
Epoch 40/1000, Loss: 16.08851107954979
Epoch 41/1000, Loss: 15.723034292459488
Epoch 42/1000, Loss: 16.014235720038414
Epoch 43/1000, Loss: 15.87851519882679
Epoch 44/1000, Loss: 15.597006469964981
Epoch 45/1000, Loss: 16.108567476272583
Epoch 46/1000, Loss: 16.12450796365738
Epoch 47/1000, Loss: 15.725275844335556
Epoch 48/1000, Loss: 15.980055116117
Epoch 49/1000, Loss: 15.548039868474007
Epoch 50/1000, Loss: 15.980508998036385
Epoch 51/1000, Loss: 15.729415237903595
Epoch 52/1000, Loss: 15.627949476242065
Epoch 53/1000, Loss: 15.561315074563026
Epoch 54/1000, Loss: 15.712111696600914
Epoch 55/1000, Loss: 15.63821892440319
Epoch 56/1000, Loss: 15.448936149477959
Epoch 57/1000, Loss: 15.475876480340958
Epoch 58/1000, Loss: 15.545578464865685
Epoch 59/1000, Loss: 15.725586615502834
Epoch 60/1000, Loss: 15.60325200855732
Epoch 61/1000, Loss: 15.80497919023037
Epoch 62/1000, Loss: 15.62500336766243
Epoch 63/1000, Loss: 15.772025845944881
Epoch 64/1000, Loss: 16.042719587683678
Epoch 65/1000, Loss: 15.992974661290646
Epoch 66/1000, Loss: 15.852920763194561
Epoch 67/1000, Loss: 15.580330207943916
Epoch 68/1000, Loss: 15.86386513710022
Epoch 69/1000, Loss: 15.754110559821129
Epoch 70/1000, Loss: 15.376005947589874
Epoch 71/1000, Loss: 15.917576998472214
Epoch 72/1000, Loss: 15.445948988199234
Epoch 73/1000, Loss: 15.841234974563122
Epoch 74/1000, Loss: 15.71682357788086
Epoch 75/1000, Loss: 15.756102710962296
Epoch 76/1000, Loss: 15.726014792919159
Epoch 77/1000, Loss: 15.524562895298004
Epoch 78/1000, Loss: 15.40105365216732
Epoch 79/1000, Loss: 15.376122109591961
Epoch 80/1000, Loss: 15.435616746544838
Epoch 81/1000, Loss: 15.48473185300827
Epoch 82/1000, Loss: 15.77623787522316
Epoch 83/1000, Loss: 15.608633518218994
Epoch 84/1000, Loss: 15.225082457065582
Epoch 85/1000, Loss: 15.268565937876701
Epoch 86/1000, Loss: 16.09590481221676
Epoch 87/1000, Loss: 15.265016719698906
Epoch 88/1000, Loss: 15.567040801048279
Epoch 89/1000, Loss: 15.551608055830002
Epoch 90/1000, Loss: 15.825078755617142
Epoch 91/1000, Loss: 15.796109437942505
Epoch 92/1000, Loss: 15.732715353369713
Epoch 93/1000, Loss: 15.500598400831223
Epoch 94/1000, Loss: 15.210646413266659
Epoch 95/1000, Loss: 15.193903051316738
Epoch 96/1000, Loss: 15.466841951012611
Epoch 97/1000, Loss: 15.623654648661613
Epoch 98/1000, Loss: 15.598592907190323
Epoch 99/1000, Loss: 15.639197655022144
Epoch 100/1000, Loss: 15.3035439401865
Epoch 101/1000, Loss: 15.117041736841202
Epoch 102/1000, Loss: 15.375698670744896
Epoch 103/1000, Loss: 15.547274678945541
Epoch 104/1000, Loss: 15.677918642759323
Epoch 105/1000, Loss: 15.726463705301285
Epoch 106/1000, Loss: 14.95219860970974
Epoch 107/1000, Loss: 15.609195083379745
Epoch 108/1000, Loss: 15.548822402954102
Epoch 109/1000, Loss: 15.458397645503283
Epoch 110/1000, Loss: 15.571750849485397
Epoch 111/1000, Loss: 15.491282418370247
Epoch 112/1000, Loss: 15.526455760002136
Epoch 113/1000, Loss: 15.5477951541543
Epoch 114/1000, Loss: 15.569227389991283
Epoch 115/1000, Loss: 15.593830615282059
Epoch 116/1000, Loss: 15.110083043575287
Epoch 117/1000, Loss: 15.349766716361046
Epoch 118/1000, Loss: 15.563852146267891
Epoch 119/1000, Loss: 15.453773215413094
Epoch 120/1000, Loss: 14.950620487332344
Epoch 121/1000, Loss: 15.075219750404358
Epoch 122/1000, Loss: 15.520495153963566
Epoch 123/1000, Loss: 15.431771218776703
Epoch 124/1000, Loss: 15.271775737404823
Epoch 125/1000, Loss: 15.243697956204414
Epoch 126/1000, Loss: 15.291081354022026
Epoch 127/1000, Loss: 15.227356851100922
Epoch 128/1000, Loss: 15.260050475597382
Epoch 129/1000, Loss: 15.77521488070488
Epoch 130/1000, Loss: 15.461004167795181
Epoch 131/1000, Loss: 15.483161315321922
Epoch 132/1000, Loss: 15.650799289345741
Epoch 133/1000, Loss: 15.067580983042717
Epoch 134/1000, Loss: 15.686539582908154
Epoch 135/1000, Loss: 15.414016872644424
Epoch 136/1000, Loss: 14.84942215681076
Epoch 137/1000, Loss: 15.412696227431297
Epoch 138/1000, Loss: 15.509814292192459
Epoch 139/1000, Loss: 15.375043272972107
Epoch 140/1000, Loss: 15.608532279729843
Epoch 141/1000, Loss: 15.538037523627281
Epoch 142/1000, Loss: 15.700071923434734
Epoch 143/1000, Loss: 15.66982764005661
Epoch 144/1000, Loss: 15.539854489266872
Epoch 145/1000, Loss: 14.886452212929726
Epoch 146/1000, Loss: 15.258946418762207
Epoch 147/1000, Loss: 15.384785503149033
Epoch 148/1000, Loss: 15.060863889753819
Epoch 149/1000, Loss: 15.768676742911339
Epoch 150/1000, Loss: 15.426471777260303
Epoch 151/1000, Loss: 15.357111662626266
Epoch 152/1000, Loss: 15.547743648290634
Epoch 153/1000, Loss: 15.47683173418045
Epoch 154/1000, Loss: 15.073350295424461
Epoch 155/1000, Loss: 15.407103598117828
Epoch 156/1000, Loss: 15.67839141190052
Epoch 157/1000, Loss: 15.371790409088135
Epoch 158/1000, Loss: 15.172465145587921
Epoch 159/1000, Loss: 15.326018318533897
Epoch 160/1000, Loss: 15.212202191352844
Epoch 161/1000, Loss: 15.46677878499031
Epoch 162/1000, Loss: 15.56594680249691
Epoch 163/1000, Loss: 15.21596810221672
Epoch 164/1000, Loss: 15.31818588078022
Epoch 165/1000, Loss: 15.692747823894024
Epoch 166/1000, Loss: 15.366563059389591
Epoch 167/1000, Loss: 15.644924700260162
Epoch 168/1000, Loss: 15.46385783702135
Epoch 169/1000, Loss: 15.321651041507721
Epoch 170/1000, Loss: 15.134643368422985
Epoch 171/1000, Loss: 15.399787530303001
Epoch 172/1000, Loss: 15.832462787628174
Epoch 173/1000, Loss: 15.505942270159721
Epoch 174/1000, Loss: 15.492646649479866
Epoch 175/1000, Loss: 15.918539077043533
Epoch 176/1000, Loss: 15.665008664131165
Epoch 177/1000, Loss: 15.269533261656761
Epoch 178/1000, Loss: 15.554136097431183
Epoch 179/1000, Loss: 15.671808332204819
Epoch 180/1000, Loss: 15.02529264986515
Epoch 181/1000, Loss: 15.533827483654022
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 86, in <module>
    fit(model, optimizer, dataloader, max_epoch=1000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 60, in fit
    l.backward()
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt