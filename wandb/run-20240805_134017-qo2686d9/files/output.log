/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/100, Loss: 162.6417269706726
Epoch 2/100, Loss: 160.4818902015686
Epoch 3/100, Loss: 160.81660616397858
Epoch 4/100, Loss: 164.65689182281494
Epoch 5/100, Loss: 164.18066084384918
Epoch 6/100, Loss: 162.55381834506989
Epoch 7/100, Loss: 167.51644015312195
Epoch 8/100, Loss: 163.76484513282776
Epoch 9/100, Loss: 157.82574439048767
Epoch 10/100, Loss: 162.8577460050583
Epoch 11/100, Loss: 161.0107011795044
Epoch 12/100, Loss: 164.07275700569153
Epoch 13/100, Loss: 164.38598442077637
Epoch 14/100, Loss: 168.3225953578949
Epoch 15/100, Loss: 161.04152047634125
Epoch 16/100, Loss: 163.2600452899933
Epoch 17/100, Loss: 161.10281133651733
Epoch 18/100, Loss: 157.44660830497742
Epoch 19/100, Loss: 162.41684293746948
Epoch 20/100, Loss: 156.97835731506348
Epoch 21/100, Loss: 161.10174024105072
Epoch 22/100, Loss: 158.0720533132553
Epoch 23/100, Loss: 165.13497722148895
Epoch 24/100, Loss: 158.97834300994873
Epoch 25/100, Loss: 163.38440203666687
Epoch 26/100, Loss: 158.47683453559875
Epoch 27/100, Loss: 167.00733745098114
Epoch 28/100, Loss: 167.57259702682495
Epoch 29/100, Loss: 158.63396430015564
Epoch 30/100, Loss: 162.5714454650879
Epoch 31/100, Loss: 164.38388681411743
Epoch 32/100, Loss: 158.91531467437744
Epoch 33/100, Loss: 160.75864136219025
Epoch 34/100, Loss: 158.19562363624573
Epoch 35/100, Loss: 154.69592475891113
Epoch 36/100, Loss: 160.07200384140015
Epoch 37/100, Loss: 158.69622147083282
Epoch 38/100, Loss: 158.3517234325409
Epoch 39/100, Loss: 159.82212114334106
Epoch 40/100, Loss: 155.6020007133484
Epoch 41/100, Loss: 162.07039141654968
Epoch 42/100, Loss: 155.19537472724915
Epoch 43/100, Loss: 161.9139095544815
Epoch 44/100, Loss: 162.44447088241577
Epoch 45/100, Loss: 160.2888514995575
Epoch 46/100, Loss: 158.5080599784851
Epoch 47/100, Loss: 157.66366922855377
Epoch 48/100, Loss: 162.41309303045273
Epoch 49/100, Loss: 162.9146202802658
Epoch 50/100, Loss: 158.47611784934998
Epoch 51/100, Loss: 159.82077479362488
Epoch 52/100, Loss: 162.81958150863647
Epoch 53/100, Loss: 157.66396176815033
Epoch 54/100, Loss: 157.0385469198227
Epoch 55/100, Loss: 160.38169920444489
Epoch 56/100, Loss: 155.56948018074036
Epoch 57/100, Loss: 163.2269241809845
Epoch 58/100, Loss: 157.19411611557007
Epoch 59/100, Loss: 159.85086011886597
Epoch 60/100, Loss: 157.0393147468567
Epoch 61/100, Loss: 157.0695527791977
Epoch 62/100, Loss: 156.10070896148682
Epoch 63/100, Loss: 162.82006108760834
Epoch 64/100, Loss: 154.9441065788269
Epoch 65/100, Loss: 158.9127712249756
Epoch 66/100, Loss: 156.10150289535522
Epoch 67/100, Loss: 160.7260036468506
Epoch 68/100, Loss: 155.6322250366211
Epoch 69/100, Loss: 156.63202726840973
Epoch 70/100, Loss: 163.6946840286255
Epoch 71/100, Loss: 158.94449150562286
Epoch 72/100, Loss: 159.66474199295044
Epoch 73/100, Loss: 155.60038590431213
Epoch 74/100, Loss: 160.53884780406952
Epoch 75/100, Loss: 157.8189219236374
Epoch 76/100, Loss: 158.28960716724396
Epoch 77/100, Loss: 157.66252195835114
Epoch 78/100, Loss: 154.31959795951843
Epoch 79/100, Loss: 154.72507560253143
Epoch 80/100, Loss: 158.53764986991882
Epoch 81/100, Loss: 157.1633496284485
Epoch 82/100, Loss: 161.47546517848969
Epoch 83/100, Loss: 160.41348612308502
Epoch 84/100, Loss: 159.6004754304886
Epoch 85/100, Loss: 158.94437563419342
Epoch 86/100, Loss: 152.69281697273254
Epoch 87/100, Loss: 158.5378918647766
Epoch 88/100, Loss: 152.91350483894348
Epoch 89/100, Loss: 155.38117957115173
Epoch 90/100, Loss: 157.88138675689697
Epoch 91/100, Loss: 153.78866291046143
Epoch 92/100, Loss: 159.38139379024506
Epoch 93/100, Loss: 158.1636905670166
Epoch 94/100, Loss: 155.53767108917236
Epoch 95/100, Loss: 158.41327250003815
Epoch 96/100, Loss: 155.22539138793945
Epoch 97/100, Loss: 154.22564232349396
Epoch 98/100, Loss: 157.35061419010162
Epoch 99/100, Loss: 156.7873990535736
Epoch 100/100, Loss: 157.22490429878235