/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/1000, Loss: 6.192565582692623
Epoch 2/1000, Loss: 6.602920949459076
Epoch 3/1000, Loss: 10.17766185104847
Epoch 4/1000, Loss: 6.884695820510387
Epoch 5/1000, Loss: 6.089669659733772
Epoch 6/1000, Loss: 5.819289531558752
Epoch 7/1000, Loss: 5.892690494656563
Epoch 8/1000, Loss: 5.8642726093530655
Epoch 9/1000, Loss: 5.8134406208992
Epoch 10/1000, Loss: 5.820609852671623
Epoch 11/1000, Loss: 5.792905304580927
Epoch 12/1000, Loss: 5.849685795605183
Epoch 13/1000, Loss: 5.8651344776153564
Epoch 14/1000, Loss: 5.865427054464817
Epoch 15/1000, Loss: 5.90599974244833
Epoch 16/1000, Loss: 5.857095442712307
Epoch 17/1000, Loss: 5.911303963512182
Epoch 18/1000, Loss: 5.7968235574662685
Epoch 19/1000, Loss: 5.850245337933302
Epoch 20/1000, Loss: 5.803681768476963
Epoch 21/1000, Loss: 5.879363194108009
Epoch 22/1000, Loss: 5.776602916419506
Epoch 23/1000, Loss: 5.909696772694588
Epoch 24/1000, Loss: 5.841771125793457
Epoch 25/1000, Loss: 5.876508645713329
Epoch 26/1000, Loss: 5.855418890714645
Epoch 27/1000, Loss: 5.835676666349173
Epoch 28/1000, Loss: 5.874038688838482
Epoch 29/1000, Loss: 5.857569873332977
Epoch 30/1000, Loss: 5.819511532783508
Epoch 31/1000, Loss: 5.87345876917243
Epoch 32/1000, Loss: 5.896872844547033
Epoch 33/1000, Loss: 5.894024953246117
Epoch 34/1000, Loss: 5.8683685548603535
Epoch 35/1000, Loss: 5.812142796814442
Epoch 36/1000, Loss: 5.937914550304413
Epoch 37/1000, Loss: 5.915980868041515
Epoch 38/1000, Loss: 5.893927242606878
Epoch 39/1000, Loss: 5.923862498253584
Epoch 40/1000, Loss: 5.855384148657322
Epoch 41/1000, Loss: 5.918448448181152
Epoch 42/1000, Loss: 5.941324956715107
Epoch 43/1000, Loss: 5.803690955042839
Epoch 44/1000, Loss: 5.914923906326294
Epoch 45/1000, Loss: 5.876764379441738
Epoch 46/1000, Loss: 5.857987154275179
Epoch 47/1000, Loss: 5.939980335533619
Epoch 48/1000, Loss: 5.780631318688393
Epoch 49/1000, Loss: 5.846206195652485
Epoch 50/1000, Loss: 5.925397150218487
Epoch 51/1000, Loss: 5.86332955211401
Epoch 52/1000, Loss: 5.893665537238121
Epoch 53/1000, Loss: 5.880693681538105
Epoch 54/1000, Loss: 5.941961951553822
Epoch 55/1000, Loss: 5.917753614485264
Epoch 56/1000, Loss: 5.839436959475279
Epoch 57/1000, Loss: 5.944873183965683
Epoch 58/1000, Loss: 5.917526829987764
Epoch 59/1000, Loss: 5.874065488576889
Epoch 60/1000, Loss: 5.904018774628639
Epoch 61/1000, Loss: 5.966225441545248
Epoch 62/1000, Loss: 5.885167628526688
Epoch 63/1000, Loss: 5.979349330067635
Epoch 64/1000, Loss: 5.955786865204573
Epoch 65/1000, Loss: 5.883296694606543
Epoch 66/1000, Loss: 5.938891030848026
Epoch 67/1000, Loss: 5.903946816921234
Epoch 68/1000, Loss: 5.859626032412052
Epoch 69/1000, Loss: 5.864694260060787
Epoch 70/1000, Loss: 5.856046047061682
Epoch 71/1000, Loss: 5.940055564045906
Epoch 72/1000, Loss: 5.979218348860741
Epoch 73/1000, Loss: 5.843866631388664
Epoch 74/1000, Loss: 5.8593483828008175
Epoch 75/1000, Loss: 5.919525135308504
Epoch 76/1000, Loss: 5.916078701615334
Epoch 77/1000, Loss: 5.928494665771723
Epoch 78/1000, Loss: 5.905763246119022
Epoch 79/1000, Loss: 5.9446993842720985
Epoch 80/1000, Loss: 5.949964039027691
Epoch 81/1000, Loss: 5.836311936378479
Epoch 82/1000, Loss: 5.949083313345909
Epoch 83/1000, Loss: 5.896584756672382
Epoch 84/1000, Loss: 5.925031289458275
Epoch 85/1000, Loss: 5.910742364823818
Epoch 86/1000, Loss: 5.985303722321987
Epoch 87/1000, Loss: 5.938110377639532
Epoch 88/1000, Loss: 5.9730374366045
Epoch 89/1000, Loss: 5.970304101705551
Epoch 90/1000, Loss: 5.937163904309273
Epoch 91/1000, Loss: 5.932592540979385
Epoch 92/1000, Loss: 5.936737455427647
Epoch 93/1000, Loss: 5.946232661604881
Epoch 94/1000, Loss: 5.8931165263056755
Epoch 95/1000, Loss: 6.012940920889378
Epoch 96/1000, Loss: 5.882464520633221
Epoch 97/1000, Loss: 5.919826742261648
Epoch 98/1000, Loss: 5.9268993474543095
Epoch 99/1000, Loss: 5.895711190998554
Epoch 100/1000, Loss: 5.920992411673069
Epoch 101/1000, Loss: 5.891683794558048
Epoch 102/1000, Loss: 5.958530493080616
Epoch 103/1000, Loss: 6.009863283485174
Epoch 104/1000, Loss: 5.970578204840422
Epoch 105/1000, Loss: 5.966938715428114
Epoch 106/1000, Loss: 5.979135498404503
Epoch 107/1000, Loss: 5.926823128014803
Epoch 108/1000, Loss: 5.972529862076044
Epoch 109/1000, Loss: 5.945812348276377
Epoch 110/1000, Loss: 5.891584195196629
Epoch 111/1000, Loss: 5.93699312210083
Epoch 112/1000, Loss: 5.9414079040288925
Epoch 113/1000, Loss: 5.9694538079202175
Epoch 114/1000, Loss: 5.937168937176466
Epoch 115/1000, Loss: 5.887443244457245
Epoch 116/1000, Loss: 5.940250184386969
Epoch 117/1000, Loss: 5.985404007136822
Epoch 118/1000, Loss: 5.920519229024649
Epoch 119/1000, Loss: 5.967421963810921
Epoch 120/1000, Loss: 5.980686694383621
Epoch 121/1000, Loss: 5.971599042415619
Epoch 122/1000, Loss: 5.92923017591238
Epoch 123/1000, Loss: 5.978320132941008
Epoch 124/1000, Loss: 5.972620561718941
Epoch 125/1000, Loss: 5.93331016600132
Epoch 126/1000, Loss: 5.888412997126579
Epoch 127/1000, Loss: 5.906213786453009
Epoch 128/1000, Loss: 5.993184920400381
Epoch 129/1000, Loss: 5.961135413497686
Epoch 130/1000, Loss: 5.978767599910498
Epoch 131/1000, Loss: 5.981395352631807
Epoch 132/1000, Loss: 5.876884337514639
Epoch 133/1000, Loss: 5.997557140886784
Epoch 134/1000, Loss: 5.957065582275391
Epoch 135/1000, Loss: 5.937432248145342
Epoch 136/1000, Loss: 5.922501880675554
Epoch 137/1000, Loss: 5.974113814532757
Epoch 138/1000, Loss: 5.948385633528233
Epoch 139/1000, Loss: 5.988898009061813
Epoch 140/1000, Loss: 5.934517282992601
Epoch 141/1000, Loss: 5.956968419253826
Epoch 142/1000, Loss: 6.035597559064627
Epoch 143/1000, Loss: 5.941300772130489
Epoch 144/1000, Loss: 5.903010107576847
Epoch 145/1000, Loss: 5.935725383460522
Epoch 146/1000, Loss: 5.942658513784409
Epoch 147/1000, Loss: 5.941775020211935
Epoch 148/1000, Loss: 5.9809160232543945
Epoch 149/1000, Loss: 5.926620379090309
Epoch 150/1000, Loss: 5.954690977931023
Epoch 151/1000, Loss: 5.952558036893606
Epoch 152/1000, Loss: 5.986360631883144
Epoch 153/1000, Loss: 5.929050009697676
Epoch 154/1000, Loss: 5.88575042411685
Epoch 155/1000, Loss: 5.915262561291456
Epoch 156/1000, Loss: 5.840301029384136
Epoch 157/1000, Loss: 5.99450858682394
Epoch 158/1000, Loss: 5.895195074379444
Epoch 159/1000, Loss: 5.896482340991497
Epoch 160/1000, Loss: 5.987036734819412
Epoch 161/1000, Loss: 5.987204659730196
Epoch 162/1000, Loss: 5.971191320568323
Epoch 163/1000, Loss: 5.946629993617535
Epoch 164/1000, Loss: 5.936926808208227
Epoch 165/1000, Loss: 5.930603388696909
Epoch 166/1000, Loss: 5.961805991828442
Epoch 167/1000, Loss: 5.990672767162323
Epoch 168/1000, Loss: 5.9150828421115875
Epoch 169/1000, Loss: 5.938469100743532
Epoch 170/1000, Loss: 5.9174375385046005
Epoch 171/1000, Loss: 5.965176440775394
Epoch 172/1000, Loss: 5.943170554935932
Epoch 173/1000, Loss: 5.852430056780577
Epoch 174/1000, Loss: 6.008431658148766
Epoch 175/1000, Loss: 5.922717414796352
Epoch 176/1000, Loss: 5.945490263402462
Epoch 177/1000, Loss: 5.922851145267487
Epoch 178/1000, Loss: 5.91432312130928
Epoch 179/1000, Loss: 5.938528388738632
Epoch 180/1000, Loss: 5.912177473306656
Epoch 181/1000, Loss: 5.910228453576565
Epoch 182/1000, Loss: 5.921911865472794
Epoch 183/1000, Loss: 5.99106689542532
Epoch 184/1000, Loss: 5.831661097705364
Epoch 185/1000, Loss: 5.9489719942212105
Epoch 186/1000, Loss: 6.033614680171013
Epoch 187/1000, Loss: 5.900104638189077
Epoch 188/1000, Loss: 5.938906516879797
Epoch 189/1000, Loss: 6.009980749338865
Epoch 190/1000, Loss: 5.904233284294605
Epoch 191/1000, Loss: 5.8963125720620155
Epoch 192/1000, Loss: 5.91811215877533
Epoch 193/1000, Loss: 5.91007474064827
Epoch 194/1000, Loss: 5.943311098963022
Epoch 195/1000, Loss: 5.863501094281673
Epoch 196/1000, Loss: 5.864389676600695
Epoch 197/1000, Loss: 5.93953263014555
Epoch 198/1000, Loss: 5.969146393239498
Epoch 199/1000, Loss: 5.914091903716326
Epoch 200/1000, Loss: 5.966061819344759
Epoch 201/1000, Loss: 5.954977873712778
Epoch 202/1000, Loss: 5.894066587090492
Epoch 203/1000, Loss: 6.011916846036911
Epoch 204/1000, Loss: 5.918027903884649
Epoch 205/1000, Loss: 5.964520025998354
Epoch 206/1000, Loss: 5.902311075478792
Epoch 207/1000, Loss: 5.937325619161129
Epoch 208/1000, Loss: 5.957708332687616
Epoch 209/1000, Loss: 5.924859821796417
Epoch 210/1000, Loss: 5.9295080080628395
Epoch 211/1000, Loss: 5.888664353638887
Epoch 212/1000, Loss: 5.9528060257434845
Epoch 213/1000, Loss: 5.993067994713783
Epoch 214/1000, Loss: 5.978852950036526
Epoch 215/1000, Loss: 5.9173850901424885
Epoch 216/1000, Loss: 5.927987698465586
Epoch 217/1000, Loss: 5.949043765664101
Epoch 218/1000, Loss: 5.9609008096158504
Epoch 219/1000, Loss: 5.915171399712563
Epoch 220/1000, Loss: 5.954458478838205
Epoch 221/1000, Loss: 5.940492507070303
Epoch 222/1000, Loss: 5.967945706099272
Epoch 223/1000, Loss: 5.975799288600683
Epoch 224/1000, Loss: 5.917083133012056
Epoch 225/1000, Loss: 5.993429385125637
Epoch 226/1000, Loss: 5.904921777546406
Epoch 227/1000, Loss: 5.974574223160744
Epoch 228/1000, Loss: 5.901104435324669
Epoch 229/1000, Loss: 5.928684905171394
Epoch 230/1000, Loss: 5.946030348539352
Epoch 231/1000, Loss: 5.922887168824673
Epoch 232/1000, Loss: 5.959658246487379
Epoch 233/1000, Loss: 5.953362721949816
Epoch 234/1000, Loss: 5.968611292541027
Epoch 235/1000, Loss: 5.903761528432369
Epoch 236/1000, Loss: 5.917248729616404
Epoch 237/1000, Loss: 5.906673721969128
Epoch 238/1000, Loss: 5.962401982396841
Epoch 239/1000, Loss: 5.931786518543959
Epoch 240/1000, Loss: 5.959737461060286
Epoch 241/1000, Loss: 5.850102502852678
Epoch 242/1000, Loss: 5.910237357020378
Epoch 243/1000, Loss: 5.921968474984169
Epoch 244/1000, Loss: 5.988086648285389
Epoch 245/1000, Loss: 5.920083452016115
Epoch 246/1000, Loss: 5.95570557191968
Epoch 247/1000, Loss: 5.989362750202417
Epoch 248/1000, Loss: 5.988393571227789
Epoch 249/1000, Loss: 5.944953892379999
Epoch 250/1000, Loss: 5.89097585901618
Epoch 251/1000, Loss: 6.037120286375284
Epoch 252/1000, Loss: 5.986124958842993
Epoch 253/1000, Loss: 5.930940203368664
Epoch 254/1000, Loss: 5.972765184938908
Epoch 255/1000, Loss: 5.96909012645483
Epoch 256/1000, Loss: 5.8845523446798325
Epoch 257/1000, Loss: 5.9410385973751545
Epoch 258/1000, Loss: 5.845966577529907
Epoch 259/1000, Loss: 5.996684804558754
Epoch 260/1000, Loss: 5.885582212358713
Epoch 261/1000, Loss: 5.909353896975517
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 86, in <module>
    fit(model, optimizer, dataloader, max_epoch=1000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_unet_adj_neigh.py", line 59, in fit
    l += loss_cycle_consistency(score[i].unsqueeze(0) , train_noise_adj_b_chunked.squeeze(1)[i].unsqueeze(0), sigma.item(),  device, grid_shape=grid_shape)
                                                                                                              ^^^^^^^^^^^^
KeyboardInterrupt