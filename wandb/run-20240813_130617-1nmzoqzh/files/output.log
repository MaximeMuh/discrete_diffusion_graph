/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/2000, Loss: 434.3931471033904
New best model saved with loss: 434.3931471033904
Epoch 2/2000, Loss: 160.80795211080266
New best model saved with loss: 160.80795211080266
Epoch 3/2000, Loss: 113.96586264993407
New best model saved with loss: 113.96586264993407
Epoch 4/2000, Loss: 118.17585346151732
Epoch 5/2000, Loss: 82.64872855251949
New best model saved with loss: 82.64872855251949
Epoch 6/2000, Loss: 142.1228209604558
Epoch 7/2000, Loss: 88.96432513292683
Epoch 8/2000, Loss: 126.06885017003668
Epoch 9/2000, Loss: 109.48965965422164
Epoch 10/2000, Loss: 98.17315901394345
Epoch 11/2000, Loss: 95.5422846056815
Epoch 12/2000, Loss: 159.4894717976843
Epoch 13/2000, Loss: 132.38354083376103
Epoch 14/2000, Loss: 119.7524729746541
Epoch 15/2000, Loss: 85.54170576081071
Epoch 16/2000, Loss: 130.2783126641849
Epoch 17/2000, Loss: 125.80517888314237
Epoch 18/2000, Loss: 110.27844487914913
Epoch 19/2000, Loss: 130.80508458957468
Epoch 20/2000, Loss: 100.27836435827378
Epoch 21/2000, Loss: 125.01540191035326
Epoch 22/2000, Loss: 85.0158033958195
Epoch 23/2000, Loss: 118.69815086803742
Epoch 24/2000, Loss: 119.22563884167099
Epoch 25/2000, Loss: 189.2254378507404
Epoch 26/2000, Loss: 140.0152744911611
Epoch 27/2000, Loss: 170.80464604011686
Epoch 28/2000, Loss: 128.43572659854237
Epoch 29/2000, Loss: 195.80394050047585
Epoch 30/2000, Loss: 141.8570554470153
Epoch 31/2000, Loss: 134.75162524818197
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 104, in <module>
    fit(model, optimizer, dataloader, max_epoch=2000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 56, in fit
    l.backward()
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt