/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/2000, Loss: 640.499666314376
New best model saved with loss: 640.499666314376
Epoch 2/2000, Loss: 174.65531098215203
New best model saved with loss: 174.65531098215203
Epoch 3/2000, Loss: 190.93190353795103
Epoch 4/2000, Loss: 152.36350134519958
New best model saved with loss: 152.36350134519958
Epoch 5/2000, Loss: 169.1155098362973
Epoch 6/2000, Loss: 151.74219692625888
New best model saved with loss: 151.74219692625888
Epoch 7/2000, Loss: 149.37877926073577
New best model saved with loss: 149.37877926073577
Epoch 8/2000, Loss: 144.36279979505036
New best model saved with loss: 144.36279979505036
Epoch 9/2000, Loss: 178.06820006119577
Epoch 10/2000, Loss: 177.34724185341284
Epoch 11/2000, Loss: 187.6947360791658
Epoch 12/2000, Loss: 142.0939026680334
New best model saved with loss: 142.0939026680334
Epoch 13/2000, Loss: 167.17863384046052
Epoch 14/2000, Loss: 178.02006874262895
Epoch 15/2000, Loss: 137.2996812700539
New best model saved with loss: 137.2996812700539
Epoch 16/2000, Loss: 155.9209994151698
Epoch 17/2000, Loss: 154.3571570788167
Epoch 18/2000, Loss: 190.55776094135484
Epoch 19/2000, Loss: 234.72062883663335
Epoch 20/2000, Loss: 185.6263947941755
Epoch 21/2000, Loss: 172.50951285111276
Epoch 22/2000, Loss: 191.3149027388641
Epoch 23/2000, Loss: 153.15751781446957
Epoch 24/2000, Loss: 191.88394486276727
Epoch 25/2000, Loss: 144.24132748654014
Epoch 26/2000, Loss: 149.8464711841784
Epoch 27/2000, Loss: 154.84125659340307
Epoch 28/2000, Loss: 190.26268582595023
Epoch 29/2000, Loss: 151.89457234789273
Epoch 30/2000, Loss: 166.77296386830704
Epoch 31/2000, Loss: 145.6046613392077
Epoch 32/2000, Loss: 231.39925525062964
Epoch 33/2000, Loss: 198.3411126530386
Epoch 34/2000, Loss: 168.130817011783
Epoch 35/2000, Loss: 174.82591175072287
Epoch 36/2000, Loss: 149.65717195209703
Epoch 37/2000, Loss: 168.71518498516986
Epoch 38/2000, Loss: 156.9834168584723
Epoch 39/2000, Loss: 183.46778508236534
Epoch 40/2000, Loss: 177.39996729399027
Epoch 41/2000, Loss: 155.2777096182108
Epoch 42/2000, Loss: 193.91478528474508
Epoch 43/2000, Loss: 177.81505143015008
Epoch 44/2000, Loss: 166.05133197182104
Epoch 45/2000, Loss: 178.53105178632234
Epoch 46/2000, Loss: 195.17808004507893
Epoch 47/2000, Loss: 156.7680361396388
Epoch 48/2000, Loss: 172.6306266484194
Epoch 49/2000, Loss: 165.10920244764145
Epoch 50/2000, Loss: 152.8042803312603
Epoch 51/2000, Loss: 187.25690369856986
Epoch 52/2000, Loss: 184.49372120907432
Epoch 53/2000, Loss: 159.50964777093185
Epoch 54/2000, Loss: 191.92565436112253
Epoch 55/2000, Loss: 171.30450037906044
Epoch 56/2000, Loss: 179.88885452872827
Epoch 57/2000, Loss: 173.58806163386294
Epoch 58/2000, Loss: 155.71988915446164
Epoch 59/2000, Loss: 176.72580299898982
Epoch 60/2000, Loss: 154.32547881155224
Epoch 61/2000, Loss: 207.4258111652575
Epoch 62/2000, Loss: 120.91555487423351
New best model saved with loss: 120.91555487423351
Epoch 63/2000, Loss: 160.67791755472948
Epoch 64/2000, Loss: 136.3246837172069
Epoch 65/2000, Loss: 197.00436672411467
Epoch 66/2000, Loss: 189.082681633385
Epoch 67/2000, Loss: 199.85112491406892
Epoch 68/2000, Loss: 98.76208381697927
New best model saved with loss: 98.76208381697927
Epoch 69/2000, Loss: 191.88816692954614
Epoch 70/2000, Loss: 151.4260128234562
Epoch 71/2000, Loss: 180.97763881126517
Epoch 72/2000, Loss: 158.43114842866598
Epoch 73/2000, Loss: 183.56718495017603
Epoch 74/2000, Loss: 171.7986397994192
Epoch 75/2000, Loss: 211.51984084279914
Epoch 76/2000, Loss: 183.45169468929893
Epoch 77/2000, Loss: 146.29415401659514
Epoch 78/2000, Loss: 219.73573313261332
Epoch 79/2000, Loss: 186.97343000681386
Epoch 80/2000, Loss: 168.64650676124975
Epoch 81/2000, Loss: 142.07826652015117
Epoch 82/2000, Loss: 149.03579559804578
Epoch 83/2000, Loss: 172.63082390705026
Epoch 84/2000, Loss: 136.65719633299466
Epoch 85/2000, Loss: 171.70948068719161
Epoch 86/2000, Loss: 164.7150718620243
Epoch 87/2000, Loss: 129.74664051644504
Epoch 88/2000, Loss: 157.1301003728846
Epoch 89/2000, Loss: 191.55716384084602
Epoch 90/2000, Loss: 141.71517960344883
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 104, in <module>
    fit(model, optimizer, dataloader, max_epoch=2000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 56, in fit
    l.backward()
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt