/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/2000, Loss: 5355.13234349301
New best model saved with loss: 5355.13234349301
Epoch 2/2000, Loss: 1635.9339330572832
New best model saved with loss: 1635.9339330572832
Epoch 3/2000, Loss: 1672.6333666349713
Epoch 4/2000, Loss: 1456.4846705386512
New best model saved with loss: 1456.4846705386512
Epoch 5/2000, Loss: 1867.4898604778082
Epoch 6/2000, Loss: 1598.5690894190614
Epoch 7/2000, Loss: 1900.7045545076069
Epoch 8/2000, Loss: 1582.8629213594097
Epoch 9/2000, Loss: 2126.2151051571495
Epoch 10/2000, Loss: 1657.1210113520685
Epoch 11/2000, Loss: 1851.8211669921875
Epoch 12/2000, Loss: 1720.5573570440083
Epoch 13/2000, Loss: 2015.577945106908
Epoch 14/2000, Loss: 1714.8682507966694
Epoch 15/2000, Loss: 1577.8782685936283
Epoch 16/2000, Loss: 1635.9426655016448
Epoch 17/2000, Loss: 1883.7573643734581
Epoch 18/2000, Loss: 2258.099798503675
Epoch 19/2000, Loss: 1835.9099506578948
Epoch 20/2000, Loss: 1709.563282213713
Epoch 21/2000, Loss: 1382.720620154256
New best model saved with loss: 1382.720620154256
Epoch 22/2000, Loss: 1762.1943921540912
Epoch 23/2000, Loss: 2052.3308185778164
Epoch 24/2000, Loss: 1619.7940240157277
Epoch 25/2000, Loss: 2047.709328099301
Epoch 26/2000, Loss: 2030.998679713199
Epoch 27/2000, Loss: 1889.1567001842748
Epoch 28/2000, Loss: 1899.6672038028114
Epoch 29/2000, Loss: 1478.246492084704
Epoch 30/2000, Loss: 1926.410096821032
Epoch 31/2000, Loss: 1720.477718955592
Epoch 32/2000, Loss: 1778.0572821175385
Epoch 33/2000, Loss: 1494.015261198345
Epoch 34/2000, Loss: 1635.6153433429763
Epoch 35/2000, Loss: 1345.6575274890974
New best model saved with loss: 1345.6575274890974
Epoch 36/2000, Loss: 1872.4311573654413
Traceback (most recent call last):
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 104, in <module>
    fit(model, optimizer, dataloader, max_epoch=2000, device=device)
  File "/home/mmuhleth/discrete_diffusion_graph/train_ppgn/train_ppgn_simple_adj_neigh.py", line 56, in fit
    l.backward()
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/_tensor.py", line 525, in backward
    torch.autograd.backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/__init__.py", line 267, in backward
    _engine_run_backward(
  File "/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt