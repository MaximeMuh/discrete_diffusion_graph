/home/mmuhleth/discrete_diffusion_graph/utils/graphs.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  loss_matrix = loss_matrix * (1-2*torch.tensor(sigma_list).unsqueeze(-1).unsqueeze(-2).expand(grad_log_q_noise_list.size(0),grad_log_q_noise_list.size(1),grad_log_q_noise_list.size(2)).to(device)+1.0/len(sigma_list))
/home/mmuhleth/miniconda3/envs/py38/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch 1/100, Loss: 6.394974417984486
Epoch 2/100, Loss: 6.179711099714041
Epoch 3/100, Loss: 6.211282894015312
Epoch 4/100, Loss: 6.563434731215239
Epoch 5/100, Loss: 6.552309200167656
Epoch 6/100, Loss: 6.67598507553339
Epoch 7/100, Loss: 7.033766336739063
Epoch 8/100, Loss: 7.207029461860657
Epoch 9/100, Loss: 7.866011127829552
Epoch 10/100, Loss: 8.091714058071375
Epoch 11/100, Loss: 8.455313816666603
Epoch 12/100, Loss: 8.402276791632175
Epoch 13/100, Loss: 8.419662058353424
Epoch 14/100, Loss: 8.586688473820686
Epoch 15/100, Loss: 8.611414030194283
Epoch 16/100, Loss: 8.635162606835365
Epoch 17/100, Loss: 8.468918435275555
Epoch 18/100, Loss: 8.587884403765202
Epoch 19/100, Loss: 8.600881889462471
Epoch 20/100, Loss: 8.653418373316526
Epoch 21/100, Loss: 8.7932780534029
Epoch 22/100, Loss: 9.038594737648964
Epoch 23/100, Loss: 9.307352371513844
Epoch 24/100, Loss: 9.522592782974243
Epoch 25/100, Loss: 9.636173255741596
Epoch 26/100, Loss: 9.642728261649609
Epoch 27/100, Loss: 9.657250873744488
Epoch 28/100, Loss: 9.731967862695456
Epoch 29/100, Loss: 9.801591835916042
Epoch 30/100, Loss: 9.709191851317883
Epoch 31/100, Loss: 9.579365730285645
Epoch 32/100, Loss: 9.929065942764282
Epoch 33/100, Loss: 9.446432814002037
Epoch 34/100, Loss: 9.462272785604
Epoch 35/100, Loss: 9.811603173613548
Epoch 36/100, Loss: 10.066228032112122
Epoch 37/100, Loss: 9.609780721366405
Epoch 38/100, Loss: 9.679124742746353
Epoch 39/100, Loss: 9.441596522927284
Epoch 40/100, Loss: 9.394446022808552
Epoch 41/100, Loss: 9.750737942755222
Epoch 42/100, Loss: 9.59752333164215
Epoch 43/100, Loss: 9.728677451610565
Epoch 44/100, Loss: 9.35352859646082
Epoch 45/100, Loss: 9.902826011180878
Epoch 46/100, Loss: 9.508690193295479
Epoch 47/100, Loss: 9.805842369794846
Epoch 48/100, Loss: 9.729082018136978
Epoch 49/100, Loss: 9.492995992302895
Epoch 50/100, Loss: 9.774095758795738
Epoch 51/100, Loss: 9.866201378405094
Epoch 52/100, Loss: 9.691115416586399
Epoch 53/100, Loss: 9.928522735834122
Epoch 54/100, Loss: 9.781462334096432
Epoch 55/100, Loss: 9.42483077943325
Epoch 56/100, Loss: 9.868382006883621
Epoch 57/100, Loss: 9.499064095318317
Epoch 58/100, Loss: 9.776860348880291
Epoch 59/100, Loss: 9.806294299662113
Epoch 60/100, Loss: 9.856599301099777
Epoch 61/100, Loss: 9.854249432682991
Epoch 62/100, Loss: 9.76296877861023
Epoch 63/100, Loss: 10.028281271457672
Epoch 64/100, Loss: 9.666080012917519
Epoch 65/100, Loss: 9.58987295627594
Epoch 66/100, Loss: 9.901089288294315
Epoch 67/100, Loss: 9.595398135483265
Epoch 68/100, Loss: 10.025947734713554
Epoch 69/100, Loss: 9.909301176667213
Epoch 70/100, Loss: 9.672677412629128
Epoch 71/100, Loss: 9.899992600083351
Epoch 72/100, Loss: 9.528365306556225
Epoch 73/100, Loss: 9.990772262215614
Epoch 74/100, Loss: 9.899285078048706
Epoch 75/100, Loss: 9.866046033799648
Epoch 76/100, Loss: 10.314853347837925
Epoch 77/100, Loss: 9.945966437458992
Epoch 78/100, Loss: 9.661974899470806
Epoch 79/100, Loss: 9.77153254300356
Epoch 80/100, Loss: 9.818364202976227
Epoch 81/100, Loss: 9.935730323195457
Epoch 82/100, Loss: 9.835030004382133
Epoch 83/100, Loss: 10.11465559899807
Epoch 84/100, Loss: 9.658154360949993
Epoch 85/100, Loss: 9.588560976088047
Epoch 86/100, Loss: 10.185353353619576
Epoch 87/100, Loss: 10.00912680476904
Epoch 88/100, Loss: 9.700670212507248
Epoch 89/100, Loss: 9.696659661829472
Epoch 90/100, Loss: 10.168385222554207
Epoch 91/100, Loss: 9.883642390370369
Epoch 92/100, Loss: 9.862677238881588
Epoch 93/100, Loss: 10.06821621209383
Epoch 94/100, Loss: 9.946814090013504
Epoch 95/100, Loss: 9.809865586459637
Epoch 96/100, Loss: 9.823849223554134
Epoch 97/100, Loss: 9.98714505136013
Epoch 98/100, Loss: 9.847090132534504
Epoch 99/100, Loss: 9.894013192504644
Epoch 100/100, Loss: 9.769315280020237